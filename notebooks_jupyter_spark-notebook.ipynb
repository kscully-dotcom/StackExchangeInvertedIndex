{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first saved my Data Query from Stack Exchange to my google storage bucket from the terminal of the master node.\n",
    "\n",
    "Now I will load my data into spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_df = spark.read.csv(\"gs://spark-1-dataproc/data\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will look at my DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----+----------+\n",
      "|      Id|                Tags|Score|ClosedDate|\n",
      "+--------+--------------------+-----+----------+\n",
      "|69969780|  <dataframe><julia>|    5|      null|\n",
      "|69969782|<jquery><ajax><as...|    1|      null|\n",
      "|69969783|<javascript><time...|    0|      null|\n",
      "|69969784|                null|    1|      null|\n",
      "|69969785|<python><python-3...|    0|      null|\n",
      "|69969786|<c#><visual-studi...|    0|      null|\n",
      "|69969787|                null|    0|      null|\n",
      "|69969788|   <node.js><server>|    0|      null|\n",
      "|69969789|<microsoft-graph-...|    0|      null|\n",
      "|69969791|                null|    1|      null|\n",
      "+--------+--------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stack_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50000 rows in the stack_df DataFrame.\n",
      "There are 4 columns in the stack_df DataFrame and their names are ['Id', 'Tags', 'Score', 'ClosedDate'].\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} rows in the stack_df DataFrame.\".format(stack_df.count()))\n",
    "\n",
    "print(\"There are {} columns in the stack_df DataFrame and their names are {}.\".format(len(stack_df.columns), stack_df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----+----------+\n",
      "|      Id|                Tags|Score|ClosedDate|\n",
      "+--------+--------------------+-----+----------+\n",
      "|69969780|     dataframe julia|    5|      null|\n",
      "|69969782|jquery ajax asp.n...|    1|      null|\n",
      "+--------+--------------------+-----+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I am interested in posts with no tag so I will fill all Null Values with \"None\"\n",
    "stack_df_clean = stack_df.na.fill(value='None', subset=[\"Tags\"])\n",
    "\n",
    "# Now I really don't think these characters <> \n",
    "# are going to help me much. Let's get rid of those.\n",
    "stack_df_clean = stack_df_clean.withColumn('Tags', regexp_replace('Tags', '><', ' '))\n",
    "stack_df_clean = stack_df_clean.withColumn('Tags', regexp_replace('Tags', '<', ''))\n",
    "stack_df_clean = stack_df_clean.withColumn('Tags', regexp_replace('Tags', '>', ''))\n",
    "\n",
    "stack_df_clean.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First, I only want to map the Tags. \n",
    "# So I will create a sub-dataframe of Tags.\n",
    "stack_df_tags = stack_df_clean.select('Tags') \n",
    "\n",
    "# Let's see if we can make this DataFrame into an RDD.\n",
    "from pyspark.rdd import RDD\n",
    "stack_df_rdd = stack_df_tags.rdd\n",
    "\n",
    "# and map the Keys(Tags) and values(count).\n",
    "from pyspark.sql.functions import split\n",
    "tags = stack_df_rdd.flatMap(lambda row: [(tag, 1) for tag in row[0].split()]). \\\n",
    "                reduceByKey(lambda total, count: total + count)\n",
    "\n",
    "# Let's see this as a DataFrame so we can do some ploting\n",
    "tagColumns = [\"Tag\", \"Count\"]\n",
    "df_tags = tags.toDF(tagColumns).toPandas().to_csv('gs://spark-1-dataproc/data/tmp/spark_output/tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, I only want to map the Ids to the Tags. \n",
    "# So I will create a sub-dataframe of Ids and Tags.\n",
    "stack_df_id = stack_df_clean.select('Id', \"Tags\")\n",
    "\n",
    "# Let's see if we can make this DataFrame into an RDD.\n",
    "stack_df_id_rdd = stack_df_id.rdd\n",
    "\n",
    "# and map the Keys(Tags, PostID) and values(count).\n",
    "tags_plus_id = stack_df_id_rdd.flatMap(lambda row: [((row[0], tag), 1) for tag in row[1].split()]). \\\n",
    "                    reduceByKey(lambda total, count: total + count)\n",
    "\n",
    "# Let's see this as a DataFrame so we can do some ploting\n",
    "idColumns = [\"PostId, Tag\", \"Count\"]\n",
    "df_id = tags_plus_id.toDF(idColumns).toPandas().to_csv('gs://spark-1-dataproc/data/tmp/spark_output/id.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly, I only want to map the Scores to the Tags. \n",
    "# So I will create a sub-dataframe of Scores and Tags.\n",
    "stack_tags_score = stack_df_clean.select('Tags', 'Score')\n",
    "\n",
    "# Let's see if we can make this DataFrame into an RDD.\n",
    "tag_score_rdd = stack_tags_score.rdd\n",
    "\n",
    "# and map the Keys(Tags, Scores) and values(count).\n",
    "tags_plus_scores = tag_score_rdd.flatMap(lambda row: [((tag, row[1]), 1) for tag in row[0].split()]). \\\n",
    "                    reduceByKey(lambda total, count: total + count)\n",
    "\n",
    "# Let's see this as a DataFrame so we can do some ploting\n",
    "scoreColumns = [\"Tag, Score\", \"Count\"]\n",
    "df_scores = tags_plus_scores.toDF(scoreColumns).toPandas().to_csv('gs://spark-1-dataproc/data/tmp/spark_output/scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}